---
layout: post
title: The path is the way
---

What can we find and where do they take us?

# Linear paths

Linear paths are the nice ones, no nasty bumps in the way...

### Matrix multiplication

It turns out that matrix multiplacation naturally sums over paths.

$$
\begin{align*}
y &= ABx \\
&= \begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{bmatrix}
\begin{bmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22} \\
\end{bmatrix}
\begin{bmatrix}
x_{1} \\
x_{2} \\
\end{bmatrix} \\
&=\begin{bmatrix}
a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\
a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \\
\end{bmatrix}
\begin{bmatrix}
x_{1} \\
x_{2} \\
\end{bmatrix}
\end{align*}
$$

![]({{ site.baseurl }}/images/LinearPath.png){: .center-image }

So as we can see, the top left entry of $AB_{1,1}=a_{11}b_{11} + a_{12}b_{21}$ is the sum of the two paths shown in orange.

The point is that linear neural networks can be viewed as a sum over independent paths.

<!-- Interesting similarities to graph algoriths.
Sum-product. Verterbi?
Message passing. -->

## Non-linearities

Kawaguchi/Choromasnska make bad assumptions. That paths are independent, and ??

$$
y_{j,i} = \sum_{p=1}^{\mid P_j\mid} [X_i]_{(j,p)} [Z_i]_{(j,p)} \prod_{k=1}^{H+1} w_{k,p}^{(k)} \\
$$

#### Entanglement

Entangled! $P(p_1,p_2) = P(p_1)P(p_2)$ in the linear case. But ... (want a nice graphical model pic? like explaining away?)

The intuition is that the knowledge that:
  path $m$'s ith ReLU is not activated ($z_i < 0$) tells us that other paths ReLU must not be activated.
  So the value of a path is entangled with the value of other paths.

### Conjecture

`Deep -> Wide transform`

>A deep network can be written as the sum over paths.
A sum over paths is a one layer network.

***

1. __Deep paths__
  * Deep nets have deeper paths than wide nets.
  * Increases in the depth of paths allow them to make higher order frequencies/oscillations. (represent more non linear fns)
  * Higher order frequencies/oscillations give increased representational power (per path).
2. __Weight tying across paths__
  * Deep nets have exponentially more paths than wide nets.
  * Deep nets share parameters between the many paths.
  * Weights with more shared paths can be learned faster. Weight tying across paths means each weight recieves more data.

Therefore: Deep networks are better than wide nets. They can learn more complex functions, in O(? size of net) vs O(? size of net), and can learn them faster, O(?? t) vs O(?? t) .
